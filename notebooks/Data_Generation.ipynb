{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d7b7f0e-38b6-4641-b026-3e4ca348c18e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f99a6b2-e93a-456c-8ad6-e6f234e3538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pathlib\n",
    "import PIL\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "\n",
    "from imageaugment import augment\n",
    "import PIL.Image\n",
    "import pathlib\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b8afa2-f327-412a-a76e-d35aeda4d1c3",
   "metadata": {},
   "source": [
    "# Create Noisy Templates\n",
    "\n",
    "In order to train a VAE for denoising problem, the clean templates need to have noise added. For this purpose, the `imageaugment` package is used and the transform parameters are set as below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab8e9e8-c7b2-4569-8856-e856ccaaa59e",
   "metadata": {},
   "source": [
    "## Set Transform Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723cae16-24bd-40ec-b6f1-123efdaac45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = augment.get_random_faxify(\n",
    "    gamma=(.8, 1.0),\n",
    "    angle_final=(0, 3),\n",
    "    angle_transient=(0, 3),\n",
    "    shift=(.005, .01),\n",
    "    scale=(1.0, 1.0),\n",
    "    threshold=(.65, .80),\n",
    "    brightness=(1.0, 1.3),\n",
    "    ditherprob=0.0,\n",
    "    flipprob=0.0,\n",
    "    vlineprob=.5,\n",
    "    maxvlines=2,\n",
    "    linewidth=(0.001, 0.002),\n",
    "    particledensity=(.001, .005),\n",
    "    particlesize=(.0001, .001)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14755a41-50d9-420c-8493-4e72f3434fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_faxified_templates(image_transform, image_folder_path, save_directory_path):\n",
    "    image_path_list = sorted(image_folder_path.rglob(\"*.png\"))\n",
    "    for image_path in image_path_list:\n",
    "        image = PIL.Image.open(image_path)\n",
    "        faxified = image_transform(image)\n",
    "        faxified.save(save_directory_path/ image_path.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c724b625-3c99-454a-aad5-bf1caf2b9d69",
   "metadata": {},
   "source": [
    "## Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aba2a39-4511-47fe-a16b-705cda1126fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images_folder_path = pathlib.Path(\"/home/fahad/training_data_with_bbox/train/documents\")\n",
    "training_images_save_path = pathlib.Path(\"/home/fahad/master_thesis/data/simulated_noisy_templates/train/\")\n",
    "\n",
    "save_faxified_templates(\n",
    "    image_transform=image_transform,\n",
    "    image_folder_path=training_images_folder_path,\n",
    "    save_directory_path=training_images_save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682fa13f-fefc-48f6-afb6-3f9cfc290786",
   "metadata": {},
   "source": [
    "## Create Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fb395e-894f-46ed-9572-c07f14173862",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_folder_path = pathlib.Path(\"/home/fahad/training_data_with_bbox/val/documents\")\n",
    "test_images_save_path = pathlib.Path(\"/home/fahad/master_thesis/data/simulated_noisy_templates/val/\")\n",
    "\n",
    "save_faxified_templates(\n",
    "    image_transform=image_transform,\n",
    "    image_folder_path=test_images_folder_path,\n",
    "    save_directory_path=test_images_save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f64dc75-559b-46e0-a899-3e0b3397e0fc",
   "metadata": {},
   "source": [
    "# Create Crops\n",
    "\n",
    "For a dataset where only handwritten crops are used for denoising application, we extract the crops from the already available noisy templates and train them against the clean counterpart. Both the noisy and clean template samples are used for extraction of the crops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4a4bd2-4946-41d2-8298-08baf78fd48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_crops(image_folder_path, graph_annotation_folder_path, save_directory, clean_crop):\n",
    "    images = sorted(image_folder_path.rglob(\"*.png\"))\n",
    "    graph_annotations = sorted(graph_annotation_folder_path.rglob(\"*.json\"))\n",
    "    index = 0\n",
    "    for image_path, annotation_path in zip(images, graph_annotations):\n",
    "        image = PIL.Image.open(image_path)\n",
    "        with open(annotation_path) as f:\n",
    "            annotations = json.load(f)\n",
    "        for annotation in annotations[\"NODES\"]:\n",
    "            if annotation[\"category\"]==\"numeric\":\n",
    "                x_top_left = annotation['origin_x']\n",
    "                y_top_left = annotation['origin_y']\n",
    "                x_bottom_right = annotation['origin_x'] + annotation['width']\n",
    "                y_bottom_right = annotation['origin_y'] + annotation['height']\n",
    "                crop = image.crop((x_top_left, y_top_left, x_bottom_right, y_bottom_right))\n",
    "                crop = crop.resize((150,100))\n",
    "                if clean_crop:\n",
    "                    crop = crop.convert('1')\n",
    "                crop.save(save_directory + str(index) + \".png\")\n",
    "                index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25674ecf-b910-4136-aa5a-55ffee25ba21",
   "metadata": {},
   "source": [
    "## Create Clean Crops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d3ef5-a972-41aa-9a18-e7b23b3f894e",
   "metadata": {},
   "source": [
    "### Training Crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e59f1bf-e8b3-413b-a1c6-5cc39d7a7e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder_path = pathlib.Path(\"/home/fahad/training_data_with_bbox/train/documents\")\n",
    "graph_annotation_folder_path = pathlib.Path(\"/home/fahad/training_data_with_bbox/train/graph_annotations/\")\n",
    "save_directory = \"/home/fahad/master_thesis/data/crops/clean_crops/train/\"\n",
    "create_crops(\n",
    "    image_folder_path=image_folder_path,\n",
    "    graph_annotation_folder_path=graph_annotation_folder_path,\n",
    "    save_directory=save_directory,\n",
    "    clean_crop=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3de168b-6dbc-4b3d-9ccf-bc08de3e9863",
   "metadata": {},
   "source": [
    "### Test Crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932c199a-f983-4749-8a0b-8efc08e58926",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder_path = pathlib.Path(\"/home/fahad/training_data_with_bbox/val/documents\")\n",
    "graph_annotation_folder_path = pathlib.Path(\"/home/fahad/training_data_with_bbox/val/graph_annotations/\")\n",
    "save_directory = \"/home/fahad/master_thesis/data/crops/clean_crops/val/\"\n",
    "create_crops(\n",
    "    image_folder_path=image_folder_path,\n",
    "    graph_annotation_folder_path=graph_annotation_folder_path,\n",
    "    save_directory=save_directory,\n",
    "    clean_crop=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8a790b-018a-4f0b-99e3-7677ad6c7cd6",
   "metadata": {},
   "source": [
    "## Create Noisy Crops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd7c6eb-6bb6-42fa-8795-f7e4b072b083",
   "metadata": {},
   "source": [
    "### Training Crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c2e71-a59f-489f-96e4-09309db34861",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = pathlib.Path(\"/home/fahad/master_thesis/data/simulated_noisy_templates/train/\")\n",
    "graph_annotation_folder = pathlib.Path(\"/home/fahad/training_data_with_bbox/train/graph_annotations/\")\n",
    "save_directory = \"/home/fahad/master_thesis/data/crops/noisy_crops/train/\"\n",
    "create_crops(\n",
    "    image_folder_path=image_folder_path,\n",
    "    graph_annotation_folder_path=graph_annotation_folder_path,\n",
    "    save_directory=save_directory,\n",
    "    clean_crop=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738239f6-6d76-4e1f-949d-db8c01f5794f",
   "metadata": {},
   "source": [
    "### Test Crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e8a116-70aa-44ae-85a3-1c31781f9cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = pathlib.Path(\"/home/fahad/master_thesis/data/simulated_noisy_templates/val/\")\n",
    "graph_annotation_folder = pathlib.Path(\"/home/fahad/training_data_with_bbox/val/graph_annotations/\")\n",
    "save_directory = \"/home/fahad/master_thesis/data/crops/noisy_crops/val/\"\n",
    "create_crops(\n",
    "    image_folder_path=image_folder_path,\n",
    "    graph_annotation_folder_path=graph_annotation_folder_path,\n",
    "    save_directory=save_directory,\n",
    "    clean_crop=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
