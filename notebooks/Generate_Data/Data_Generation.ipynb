{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "piano-authority",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "common-location",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pathlib\n",
    "import PIL\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "\n",
    "from imageaugment import augment\n",
    "import PIL.Image\n",
    "import pathlib\n",
    "import json\n",
    "import numpy as np\n",
    "import skimage\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-bahamas",
   "metadata": {},
   "source": [
    "# Create Noisy Templates\n",
    "\n",
    "In order to train a VAE for denoising problem, the clean templates need to have noise added. For this purpose, the `imageaugment` package is used and the transform parameters are set as below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-semiconductor",
   "metadata": {},
   "source": [
    "## Set Transform Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "removable-lancaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = augment.get_random_faxify(\n",
    "    gamma=(.8, 1.0),\n",
    "    #angle_final=(0, 3),\n",
    "    #angle_transient=(0, 3),\n",
    "    #shift=(.005, .01),\n",
    "    angle_final=(0, 0),\n",
    "    angle_transient=(0, 0),\n",
    "    shift=(0, 0),\n",
    "    scale=(1.0, 1.0),\n",
    "    threshold=(.65, .80),\n",
    "    brightness=(1.0, 1.3),\n",
    "    ditherprob=0.0,\n",
    "    flipprob=0.0,\n",
    "    vlineprob=.5,\n",
    "    maxvlines=2,\n",
    "    linewidth=(0.001, 0.002),\n",
    "    particledensity=(.001, .01),\n",
    "    particlesize=(.0001, .001)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "mighty-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_faxified_templates(image_transform, image_folder_path, save_directory_path, n_samples, add_gaussian=False):\n",
    "    image_path_list = sorted(image_folder_path.rglob(\"*.png\"))\n",
    "    image_path_list = image_path_list[0:n_samples]\n",
    "    for image_path in image_path_list:\n",
    "        image = PIL.Image.open(image_path).convert(\"L\")\n",
    "        faxified = image_transform(image)\n",
    "        #os.remove(image_path, *, dir_fd=None)\n",
    "        if add_gaussian:\n",
    "            faxified = PIL.Image.fromarray((skimage.util.random_noise(np.array(faxified), mode='gaussian', seed=None, clip=True, var=.05))*255).convert(\"L\")\n",
    "        faxified.save(save_directory_path/ image_path.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-toronto",
   "metadata": {},
   "source": [
    "## Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ranking-diamond",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images_folder_path = pathlib.Path(\"/home/fahad/master_thesis/data/1024x1536/simulated_clean_templates/train/\")\n",
    "training_images_save_path = pathlib.Path(\"/home/fahad/master_thesis/data/1024x1536/simulated_noisy_templates_without_gaussian_noise/train/\")\n",
    "\n",
    "save_faxified_templates(\n",
    "    image_transform=image_transform,\n",
    "    image_folder_path=training_images_folder_path,\n",
    "    save_directory_path=training_images_save_path,\n",
    "    n_samples=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-samba",
   "metadata": {},
   "source": [
    "## Create Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "opened-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images_folder_path = pathlib.Path(\"/home/fahad/master_thesis/data/1024x1536/simulated_clean_templates/val/\")\n",
    "val_images_save_path = pathlib.Path(\"/home/fahad/master_thesis/data/1024x1536/simulated_noisy_templates_without_gaussian_noise/val/\")\n",
    "\n",
    "save_faxified_templates(\n",
    "    image_transform=image_transform,\n",
    "    image_folder_path=test_images_folder_path,\n",
    "    save_directory_path=test_images_save_path,\n",
    "    n_samples=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-oregon",
   "metadata": {},
   "source": [
    "# Create Crops\n",
    "\n",
    "For a dataset where only handwritten crops are used for denoising application, we extract the crops from the already available noisy templates and train them against the clean counterpart. Both the noisy and clean template samples are used for extraction of the crops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-local",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_crops(image_folder_path, graph_annotation_folder_path, save_directory, clean_crop):\n",
    "    images = sorted(image_folder_path.rglob(\"*.png\"))\n",
    "    graph_annotations = sorted(graph_annotation_folder_path.rglob(\"*.json\"))\n",
    "    index = 0\n",
    "    for image_path, annotation_path in zip(images, graph_annotations):\n",
    "        image = PIL.Image.open(image_path)\n",
    "        with open(annotation_path) as f:\n",
    "            annotations = json.load(f)\n",
    "        for annotation in annotations[\"NODES\"]:\n",
    "            if annotation[\"category\"]==\"numeric\":\n",
    "                x_top_left = annotation['origin_x']\n",
    "                y_top_left = annotation['origin_y']\n",
    "                x_bottom_right = annotation['origin_x'] + annotation['width']\n",
    "                y_bottom_right = annotation['origin_y'] + annotation['height']\n",
    "                crop = image.crop((x_top_left, y_top_left, x_bottom_right, y_bottom_right))\n",
    "                crop = crop.resize((150,100))\n",
    "                if clean_crop:\n",
    "                    crop = crop.convert('1')\n",
    "                crop.save(save_directory + str(index) + \".png\")\n",
    "                index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-iraqi",
   "metadata": {},
   "source": [
    "## Create Clean Crops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-bibliography",
   "metadata": {},
   "source": [
    "### Training Crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-ontario",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder_path = pathlib.Path(\"/home/fahad/training_data_with_bbox/train/documents\")\n",
    "graph_annotation_folder_path = pathlib.Path(\"/home/fahad/training_data_with_bbox/train/graph_annotations/\")\n",
    "save_directory = \"/home/fahad/master_thesis/data/crops/clean_crops/train/\"\n",
    "create_crops(\n",
    "    image_folder_path=image_folder_path,\n",
    "    graph_annotation_folder_path=graph_annotation_folder_path,\n",
    "    save_directory=save_directory,\n",
    "    clean_crop=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-exercise",
   "metadata": {},
   "source": [
    "### Validation Crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder_path = pathlib.Path(\"/home/fahad/training_data_with_bbox/val/documents\")\n",
    "graph_annotation_folder_path = pathlib.Path(\"/home/fahad/training_data_with_bbox/val/graph_annotations/\")\n",
    "save_directory = \"/home/fahad/master_thesis/data/crops/clean_crops/val/\"\n",
    "create_crops(\n",
    "    image_folder_path=image_folder_path,\n",
    "    graph_annotation_folder_path=graph_annotation_folder_path,\n",
    "    save_directory=save_directory,\n",
    "    clean_crop=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-timothy",
   "metadata": {},
   "source": [
    "## Create Noisy Crops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-delivery",
   "metadata": {},
   "source": [
    "### Training Crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-invitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = pathlib.Path(\"/home/fahad/master_thesis/data/simulated_noisy_templates/train/\")\n",
    "graph_annotation_folder = pathlib.Path(\"/home/fahad/training_data_with_bbox/train/graph_annotations/\")\n",
    "save_directory = \"/home/fahad/master_thesis/data/crops/noisy_crops/train/\"\n",
    "create_crops(\n",
    "    image_folder_path=image_folder_path,\n",
    "    graph_annotation_folder_path=graph_annotation_folder_path,\n",
    "    save_directory=save_directory,\n",
    "    clean_crop=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-player",
   "metadata": {},
   "source": [
    "### Validation Crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = pathlib.Path(\"/home/fahad/master_thesis/data/simulated_noisy_templates/val/\")\n",
    "graph_annotation_folder = pathlib.Path(\"/home/fahad/training_data_with_bbox/val/graph_annotations/\")\n",
    "save_directory = \"/home/fahad/master_thesis/data/crops/noisy_crops/val/\"\n",
    "create_crops(\n",
    "    image_folder_path=image_folder_path,\n",
    "    graph_annotation_folder_path=graph_annotation_folder_path,\n",
    "    save_directory=save_directory,\n",
    "    clean_crop=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
