{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2657085f-0544-4d7f-9b1e-5952204589bd",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac82eb18-9660-4081-8da1-c4c15a2bcc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "from torchvision import datasets, transforms\n",
    "from src.models import train_model, model, vanilla_model\n",
    "\n",
    "from src.features import utils\n",
    "import pathlib\n",
    "import PIL\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "\n",
    "from imageaugment import augment\n",
    "import PIL.Image\n",
    "import pathlib\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ad7354-277a-4ab3-b760-2feb76f9fa2a",
   "metadata": {},
   "source": [
    "# Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d0dff27-f2b8-458b-baf5-5519f84abf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('../../DivNoising/examples/data', train=True, transform=transforms.ToTensor()), batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80fd56fd-d841-4d37-855a-4a3639689096",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(datasets.MNIST('../../DivNoising/examples/data', train=False, transform=transforms.ToTensor()), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917ffa2-0d58-4f0a-81e1-9e4b39a5cb9c",
   "metadata": {},
   "source": [
    "## Get Mean & STD of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5185772d-5434-44a7-af1e-f0126ee86613",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.MNIST('../../DivNoising/examples/data')\n",
    "mean = 0.\n",
    "std = 0.\n",
    "for image, _ in dataset:\n",
    "    mean += np.array(image).mean()\n",
    "    std += np.array(image).std()\n",
    "\n",
    "data_mean /= len(dataset)\n",
    "data_std /= len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdeff82-b5d0-4733-9ce3-5f420cd39c68",
   "metadata": {},
   "source": [
    "# Load Simulated Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68ec0ea8-5a69-42dc-b248-2efd772af272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patches(image_folder_path):\n",
    "    image_path_list = sorted(image_folder_path.rglob(\"*.png\"))\n",
    "    images = []\n",
    "\n",
    "    for _, image_path in enumerate(image_path_list):\n",
    "        image = PIL.Image.open(image_path)\n",
    "        image = image.resize((2400, 3500))\n",
    "        image = image.convert(\"L\")\n",
    "        x = ToTensor()(image)\n",
    "\n",
    "        kh, kw = 192, 128  # kernel size\n",
    "        dh, dw = 192, 128 # stride\n",
    "        # Pad to multiples of given number\n",
    "        w_pad1 = (kw - (x.size(2)%kw)) // 2\n",
    "        w_pad2 = (kw - (x.size(2)%kw)) - w_pad1\n",
    "        h_pad1 = (kh - (x.size(1)%kh)) // 2\n",
    "        h_pad2 = (kh - (x.size(1)%kh)) - h_pad1\n",
    "        x = F.pad(x, (w_pad1, w_pad2, h_pad1, h_pad2), value=1)\n",
    "\n",
    "        patches = x.unfold(1, kh, dh).unfold(2, kw, dw)\n",
    "        #unfold_shape = patches.size()\n",
    "        patches = patches.contiguous().view(-1, kh, kw)\n",
    "        images.append(patches)\n",
    "\n",
    "    patched_image_tensors = torch.stack(images)\n",
    "    patched_images = patched_image_tensors.view(-1, 1, patched_image_tensors.size(2), patched_image_tensors.size(3))\n",
    "    \n",
    "    return patched_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8c14ab-bc94-4f1e-bf83-479025728f15",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a1a410c-d669-41e3-803f-599ae2fdd4f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([43320, 1, 192, 128])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_image_folder_train = pathlib.Path(\"/home/fahad/training_data_with_bbox/train/documents/\")\n",
    "patched_training_clean_images = load_patches(image_folder_path=clean_image_folder_train)\n",
    "\n",
    "noisy_image_folder_train = pathlib.Path(\"/home/fahad/master_thesis/data/simulated_noisy_templates/train/\")\n",
    "patched_training_noisy_images = load_patches(image_folder_path=noisy_image_folder_train)\n",
    "\n",
    "patched_training_clean_images.shape\n",
    "patched_training_noisy_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc08cc02-d5b1-4e0c-bc24-5e967517c807",
   "metadata": {},
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e114d56-8999-47f7-a15e-14d316199893",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_image_folder_test = pathlib.Path(\"/home/fahad/training_data_with_bbox/val/documents/\")\n",
    "patched_test_clean_images = load_patches(image_folder_path=clean_image_folder_test)\n",
    "\n",
    "noisy_image_folder_test = pathlib.Path(\"/home/fahad/master_thesis/data/simulated_noisy_templates/val/\")\n",
    "patched_test_noisy_images = load_patches(image_folder_path=noisy_image_folder_test)\n",
    "\n",
    "patched_test_clean_images.shape\n",
    "patched_test_noisy_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920aa380-1949-4119-af40-769d7f82f09c",
   "metadata": {},
   "source": [
    "## Check Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349127a4-1395-4b05-a562-654f3833c37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(patched_training_clean_images.shape)\n",
    "print(patched_training_noisy_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771b2631-0862-4645-afe4-e921edbca88e",
   "metadata": {},
   "source": [
    "# Create Noisy Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53dc9ce-21a8-4fdf-a79b-bd4aa716aa7e",
   "metadata": {},
   "source": [
    "## Set Transform Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "332ed3db-448a-4132-a8f2-c590da57a8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = augment.get_random_faxify(\n",
    "    gamma=(.8, 1.0),\n",
    "    angle_final=(0, 3),\n",
    "    angle_transient=(0, 3),\n",
    "    shift=(.005, .01),\n",
    "    scale=(1.0, 1.0),\n",
    "    threshold=(.65, .80),\n",
    "    brightness=(1.0, 1.3),\n",
    "    ditherprob=0.0,\n",
    "    flipprob=0.0,\n",
    "    vlineprob=.5,\n",
    "    maxvlines=2,\n",
    "    linewidth=(0.001, 0.002),\n",
    "    particledensity=(.001, .005),\n",
    "    particlesize=(.0001, .001)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a0e116-6308-4ae3-89ec-0c9a65fdc195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_faxified_templates(image_transform, image_folder_path, save_directory_path):\n",
    "    image_path_list = sorted(image_folder_path.rglob(\"*.png\"))\n",
    "    for image_path in image_path_list:\n",
    "        image = PIL.Image.open(image_path)\n",
    "        faxified = image_transform(image)\n",
    "        faxified.save(save_directory_path/ image_path.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b691aa-2bea-49b3-b1c8-e42e45ab5a86",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbc695a6-029d-495d-b259-5375fb1642fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder_val = pathlib.Path(\"/home/fahad/training_data_with_bbox/train/documents\")\n",
    "val_image_directory_path = pathlib.Path(\"/home/fahad/master_thesis/data/simulated_noisy_templates/train/\")\n",
    "\n",
    "save_faxified_templates(\n",
    "    image_transform=image_transform,\n",
    "    image_folder_path=image_folder_val,\n",
    "    save_directory_path=val_image_directory_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87615dcb-761c-41d9-a640-f543993b8299",
   "metadata": {},
   "source": [
    "## Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5d5d044-c06c-4ed8-97c7-5c86b1bcf801",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder_val = pathlib.Path(\"/home/fahad/training_data_with_bbox/val/documents\")\n",
    "val_image_directory_path = pathlib.Path(\"/home/fahad/master_thesis/data/simulated_noisy_templates/val/\")\n",
    "\n",
    "save_faxified_templates(\n",
    "    image_transform=image_transform,\n",
    "    image_folder_path=image_folder_val,\n",
    "    save_directory_path=val_image_directory_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bc5a9d-b830-48fd-ab49-71ec704d2815",
   "metadata": {},
   "source": [
    "# Create Crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d975a7-d126-4dce-81a1-37941ad60a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_crops(image_folder_path, graph_annotation_folder_path, save_directory, clean_crop):\n",
    "    images = sorted(image_folder_path.rglob(\"*.png\"))\n",
    "    graph_annotations = sorted(graph_annotation_folder_path.rglob(\"*.json\"))\n",
    "    index = 0\n",
    "    for image_path, annotation_path in zip(images, graph_annotations):\n",
    "        image = PIL.Image.open(image_path)\n",
    "        with open(annotation_path) as f:\n",
    "            annotations = json.load(f)\n",
    "        for annotation in annotations[\"NODES\"]:\n",
    "            if annotation[\"category\"]==\"numeric\":\n",
    "                x_top_left = annotation['origin_x']\n",
    "                y_top_left = annotation['origin_y']\n",
    "                x_bottom_right = annotation['origin_x'] + annotation['width']\n",
    "                y_bottom_right = annotation['origin_y'] + annotation['height']\n",
    "                crop = image.crop((x_top_left, y_top_left, x_bottom_right, y_bottom_right))\n",
    "                crop = crop.resize((150,100))\n",
    "                if clean_crop:\n",
    "                    crop = crop.convert('1')\n",
    "                crop.save(save_directory + str(index) + \".png\")\n",
    "                index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f56aa89-495c-4b11-bfa7-b4fb90d3dd1d",
   "metadata": {},
   "source": [
    "## Create Clean Crops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdd9bc6-8a9a-416a-94d6-d7093c8295b8",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a58d5d0a-08b8-40d3-b835-ebc4a1f600ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder_path = pathlib.Path(\"/home/fahad/training_data_with_bbox/train/documents\")\n",
    "graph_annotation_folder_path = pathlib.Path(\"/home/fahad/training_data_with_bbox/train/graph_annotations/\")\n",
    "save_directory = \"/home/fahad/master_thesis/data/crops/clean_crops/train/\"\n",
    "create_crops(\n",
    "    image_folder_path=image_folder_path,\n",
    "    graph_annotation_folder_path=graph_annotation_folder_path,\n",
    "    save_directory=save_directory,\n",
    "    clean_crop=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9434a3e-609a-4a1c-a378-712a6173ef38",
   "metadata": {},
   "source": [
    "### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5534a03f-f2bb-4cee-b19b-7de3d2807cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder_path = pathlib.Path(\"/home/fahad/training_data_with_bbox/val/documents\")\n",
    "graph_annotation_folder_path = pathlib.Path(\"/home/fahad/training_data_with_bbox/val/graph_annotations/\")\n",
    "save_directory = \"/home/fahad/master_thesis/data/crops/clean_crops/val/\"\n",
    "create_crops(\n",
    "    image_folder_path=image_folder_path,\n",
    "    graph_annotation_folder_path=graph_annotation_folder_path,\n",
    "    save_directory=save_directory,\n",
    "    clean_crop=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d1b1a6-b651-4db9-b3be-e6a18dff5f77",
   "metadata": {},
   "source": [
    "## Create Noisy Crops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c313126-7906-4099-8f79-dec574a46768",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "900825fb-6a3c-433d-98cf-6f6eec84cd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = pathlib.Path(\"/home/fahad/master_thesis/data/simulated_noisy_templates/train/\")\n",
    "graph_annotation_folder = pathlib.Path(\"/home/fahad/training_data_with_bbox/train/graph_annotations/\")\n",
    "save_directory = \"/home/fahad/master_thesis/data/crops/noisy_crops/train/\"\n",
    "create_crops(\n",
    "    image_folder_path=image_folder_path,\n",
    "    graph_annotation_folder_path=graph_annotation_folder_path,\n",
    "    save_directory=save_directory,\n",
    "    clean_crop=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff1c5c2-d2d1-4611-adb9-81798ba2ce91",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c997709b-3e2f-4e3a-a030-c8e59953f1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = pathlib.Path(\"/home/fahad/master_thesis/data/simulated_noisy_templates/val/\")\n",
    "graph_annotation_folder = pathlib.Path(\"/home/fahad/training_data_with_bbox/val/graph_annotations/\")\n",
    "save_directory = \"/home/fahad/master_thesis/data/crops/noisy_crops/val/\"\n",
    "create_crops(\n",
    "    image_folder_path=image_folder_path,\n",
    "    graph_annotation_folder_path=graph_annotation_folder_path,\n",
    "    save_directory=save_directory,\n",
    "    clean_crop=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98814c3c-dbe5-468a-8a00-3f00166ea8e3",
   "metadata": {},
   "source": [
    "## Load Crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af2ce80-b379-4370-a539-3f3426387a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_crops(crops_folder_path):\n",
    "    crops_path_list = sorted(crops_folder_path.rglob(\"*.png\"))\n",
    "\n",
    "    crops = [ToTensor()(PIL.Image.open(crop_path)) for crop_path in crops_path_list]\n",
    "\n",
    "    crops_tensor = torch.stack(crops)\n",
    "    crops_tensor = crops_tensor.view(-1, 1, crops_tensor.size(2), crops_tensor.size(3))\n",
    "    \n",
    "    return crops_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083d9b4f-21ab-4895-8261-6d7f8000f5ef",
   "metadata": {},
   "source": [
    "### Load Clean and Noisy Crops as Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d0b8e9c-ccf4-463b-9ecc-525bafbd8194",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_crops_folder_path = pathlib.Path(\"/home/fahad/master_thesis/data/crops/clean_crops/train/\")\n",
    "noisy_crops_folder_path = pathlib.Path(\"/home/fahad/master_thesis/data/crops/noisy_crops/train/\")\n",
    "\n",
    "training_clean_crops_data = load_crops(crops_folder_path=clean_crops_folder_path)\n",
    "training_noisy_crops_data = load_crops(crops_folder_path=noisy_crops_folder_path)\n",
    "print(training_clean_crops_data.shape)\n",
    "print(training_noisy_crops_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dc19cf-8f01-44c2-a7bc-2a1247fbb11e",
   "metadata": {},
   "source": [
    "### Load Clean and Noisy Crops as Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7c99ef-5f48-48e7-934a-55713b0fc36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_crops_folder_path = pathlib.Path(\"/home/fahad/master_thesis/data/crops/clean_crops/val/\")\n",
    "noisy_crops_folder_path = pathlib.Path(\"/home/fahad/master_thesis/data/crops/noisy_crops/val/\")\n",
    "\n",
    "validation_clean_crops_data = load_crops(crops_folder_path=clean_crops_folder_path)\n",
    "validation_noisy_crops_data = load_crops(crops_folder_path=noisy_crops_folder_path)\n",
    "print(validation_clean_crops_data.shape)\n",
    "print(validation_noisy_crops_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18231112-62db-4299-94b0-65e9ba0a4741",
   "metadata": {},
   "source": [
    "# Set Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "488869a5-201a-47d5-9817-d1eb3b80837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=361\n",
    "directory_path = \"/home/fahad/master_thesis/vanilla_vae/models/\"\n",
    "n_epochs = 50\n",
    "lr=0.001\n",
    "model_name = \"templates\"\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2860036-f0da-4976-b6ba-87f636746aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean_image_loader = DataLoader(patched_training_clean_images, batch_size=batch_size, shuffle=False)\n",
    "train_noisy_image_loader = DataLoader(patched_training_noisy_images, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efe8e59a-bbf0-4dee-99f7-488a62145ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean_image_loader = DataLoader(patched_test_clean_images, batch_size=batch_size, shuffle=False)\n",
    "test_noisy_image_loader = DataLoader(patched_test_noisy_images, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a732e9-a684-4e3a-a5a7-7f5e2d9e6bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initialize the network and the Adam optimizer\n",
    "\"\"\"\n",
    "net = vanilla_model.VAE().to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "\"\"\"\n",
    "Training the network for a given number of epochs\n",
    "The loss after every epoch is printed\n",
    "\"\"\"\n",
    "for epoch in range(n_epochs):\n",
    "    for clean_images, noisy_images in zip(train_clean_image_loader, train_noisy_image_loader):\n",
    "\n",
    "        clean_images = clean_images.to(device)\n",
    "        noisy_images = noisy_images.to(device)\n",
    "\n",
    "        # Feeding a batch of images into the network to obtain the output image, mu, and logVar\n",
    "        out, mu, logVar = net(noisy_images)\n",
    "\n",
    "        # The loss is the BCE loss combined with the KL divergence to ensure the distribution is learnt\n",
    "        kl_divergence = 0.5 * torch.sum(1 + logVar - mu.pow(2) - logVar.exp())\n",
    "        loss = F.binary_cross_entropy(out, clean_images, reduction='sum') - kl_divergence\n",
    "        #loss = torch.mean((out - data)**2) - kl_divergence\n",
    "\n",
    "        # Backpropagation based on the loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch {}: Loss {}'.format(epoch, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975caca2-68f7-4655-869d-73939c3399a3",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff996dfa-2f8b-4ae7-ab1b-67d642017b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"/home/fahad/master_thesis/vanilla_vae/models/net_final.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe0f2c3-72aa-4d74-9a5d-01f385e47335",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4268a95-b1e4-425a-bf5a-0ae7895d1a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = vanilla_model.VAE().to(device)\n",
    "net.load_state_dict(torch.load(\"/home/fahad/master_thesis/vanilla_vae/models/net_final.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d9940f-f028-4672-9843-38c8d0df5c38",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b741fd6-a685-43fe-bc07-7d6954e1b15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "clean_image_list = []\n",
    "noisy_image_list = []\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for data in random.sample(list(test_noisy_image_loader), 1):\n",
    "        imgs = data\n",
    "        for i in range(batch_size):\n",
    "            imgs = imgs.to(device)\n",
    "            img = np.transpose(imgs[i].cpu().numpy(), [1,2,0])\n",
    "            noisy_image_list.append(np.squeeze(img))\n",
    "            out, mu, logVAR = net(imgs)\n",
    "            outimg = np.transpose(out[i].cpu().numpy(), [1,2,0])\n",
    "            clean_image_list.append(np.squeeze(outimg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554712ac-b0a4-413d-9bf1-404d17c20482",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 19\n",
    "clean_image_patches = []\n",
    "noisy_image_patches = []\n",
    "for i in range(n):\n",
    "    clean_image_patches.append(np.concatenate(clean_image_list[i*n:(i+1)n], axis=1))\n",
    "    noisy_image_patches.append(np.concatenate(noisy_image_list[i*n:(i+1)n], axis=1))\n",
    "full_clean_image = np.concatenate(clean_image_patches, axis=0)\n",
    "full_noisy_image = np.concatenate(noisy_image_patches, axis=0)\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.imshow(new_image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467dc652-6c9f-417e-96b8-94d9d2c5b609",
   "metadata": {},
   "source": [
    "## Plot Input and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba598d2-fcbb-402c-b4a9-5798f0b62e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,20))\n",
    "plt.imshow(full_noisy_image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50589023-d42e-40b5-bedd-1535abd7b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,20))\n",
    "plt.imshow(full_clean_image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a7871f-75a6-4a92-8233-0e55299fcf65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
